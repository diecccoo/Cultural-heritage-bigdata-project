# ğŸ§  DEDUPLICATION PIPELINE â€“ README (sezione parziale)

Questa sezione documenta la pipeline containerizzata per la deduplicazione di oggetti culturali basata su embeddings visivi.  
La pipeline Ã¨ composta da **tre container indipendenti**, che comunicano tramite un **volume Docker condiviso** (`deduplication-data`).

---

## ğŸ“¦ Container 1: `embeddings-exporter`

### ğŸ”¹ Scopo
Estrae gli embeddings da una DeltaTable salvata su MinIO (`s3a://heritage/cleansed/embeddings/`)  
Filtra solo quelli con `embedding_status == "OK"` e salva un file `embeddings.parquet`.

### ğŸ”¹ Output
- `embeddings.parquet` scritto in `/shared-data` (cioÃ¨ nel volume `deduplication-data`)

### ğŸ”¹ Script eseguito
`spark-apps/embeddings-exporter/export_embeddings.py`

### ğŸ”¹ Dockerfile
`spark-apps/embeddings-exporter/Dockerfile`

---

## ğŸ“¦ Container 2: `faiss`

### ğŸ”¹ Scopo
Legge `embeddings.parquet`, calcola per ogni embedding immagine i `top-20` piÃ¹ simili con cosine similarity (via Faiss),  
rimuove i self-match e salva il risultato.

### ğŸ”¹ Output
- `faiss_output_topk.parquet` scritto in `/shared-data` (volume `deduplication-data`)

### ğŸ”¹ Script eseguito
`faiss/faiss_knn.py`

### ğŸ”¹ Dockerfile
`faiss/Dockerfile`

---

## ğŸ“¦ Container 3: `deduplicator-to-cleansed`

### ğŸ”¹ Scopo
Legge le coppie generate da Faiss e i vettori originali da Delta Lake,  
calcola la **cosine similarity esatta**, tiene solo le coppie con `cosine_sim â‰¥ 0.98`,  
e scrive una nuova DeltaTable contenente **solo oggetti unici** (basandosi su `min(id_object)`).

### ğŸ”¹ Output
- DeltaTable finale salvata in: `s3a://heritage/cleansed/deduplicated/`

### ğŸ”¹ Script eseguito
`spark-apps/deduplicator-to-cleansed/deduplicate_from_faiss_candidates.py`

### ğŸ”¹ Dockerfile
`spark-apps/deduplicator-to-cleansed/Dockerfile`

---

## ğŸ“ Volume condiviso: `deduplication-data`

Tutti i container montano il volume come:

```yaml
volumes:
  - deduplication-data:/shared-data
```

Il volume Ã¨ definito in fondo al `docker-compose.yml`:

```yaml
volumes:
  deduplication-data:
```

---

## â–¶ï¸ Esecuzione consigliata (in ordine)

```bash
docker-compose run --rm embeddings-exporter
docker-compose run --rm faiss
docker-compose run --rm deduplicator-to-cleansed
```

## ğŸ¤” Motivazioni delle scelte progettuali

### ğŸ”¹ PerchÃ© abbiamo usato Faiss (e non solo Spark)?
- Spark ha metodi nativi (`crossJoin`, `LSH`) per confrontare vettori, ma:
  - `crossJoin` Ã¨ molto pesante computazionalmente.
  - `LSH` Ã¨ approssimato e non consente una soglia esatta su cosine similarity.
- Faiss Ã¨ progettato per calcoli efficienti su grandi volumi di embedding e ci consente di:
  - Calcolare i `top-k` vicini per ciascun embedding immagine
  - Normalizzare per usare la cosine similarity come dot product
  - Scalare meglio in termini di prestazioni
- Abbiamo quindi scelto **Faiss + filtro Spark preciso**, per avere:
  - **efficienza** nel calcolo dei candidati
  - **precisione** con la verifica della soglia `cosine_sim â‰¥ 0.98` in Spark

---

### ğŸ”¹ PerchÃ© abbiamo salvato file intermedi come Parquet su volume Docker?
- Spark e Faiss lavorano in ambienti separati (Spark in cluster, Faiss in Python puro)
- Abbiamo bisogno di uno scambio dati tra container:
  - Salvare in **Parquet** Ã¨ leggero, veloce e ben supportato da entrambi
  - Il volume `deduplication-data` Ã¨ condiviso e garantisce persistenza tra i passaggi
- In alternativa, avremmo dovuto:
  - Scrivere/leggere da MinIO anche con Python (piÃ¹ setup)
  - O passare tutto via Kafka/Redis, che sarebbe stato **eccessivo per un task batch**

---

### ğŸ”¹ PerchÃ© abbiamo separato i container (3 servizi)?
- Ogni container ha un compito chiaro:
  - `embeddings-exporter`: estrazione da Delta Lake
  - `faiss`: calcolo top-k embedding simili
  - `deduplicator-to-cleansed`: deduplicazione con Spark
- Questa scelta offre:
  - **ModularitÃ **: puoi testare o sostituire ogni parte separatamente
  - **Pulizia architetturale**: ogni container ha uno script, uno scopo, un log
  - **FacilitÃ  di orchestrazione** futura (cronjob, Airflow, etc.)

---

### ğŸ”¹ PerchÃ© abbiamo scelto un volume Docker (`deduplication-data`) e non bind mount?
- Un volume Docker:
  - Ã¨ **isolato e riproducibile**, non dipende dal filesystem host
  - Ã¨ piÃ¹ coerente con pratiche **container-first**
  - evita problemi su Windows/macOS legati a permessi o formati file
- PoichÃ© non Ã¨ necessario ispezionare manualmente i file `.parquet`, abbiamo preferito questo approccio piÃ¹ â€œpulitoâ€ e **orientato alla produzione**

---

### ğŸ”¹ PerchÃ© abbiamo mantenuto `min(id_object)` come logica per la deduplicazione?
- Ãˆ una regola semplice, deterministica e facilmente verificabile
- Non introduce arbitrarietÃ  (es: in base a `timestamp`, `hash`, ecc.)
- Evita la necessitÃ  di salvare metadati sui duplicati scartati

---

### ğŸ”¹ PerchÃ© scriviamo la tabella finale in `overwrite`?
- Lâ€™intera deduplicazione Ã¨ batch e completa
- Sovrascrivere evita duplicati o conflitti con run precedenti
- Garantisce che la tabella `cleansed/deduplicated/` rappresenti **lo stato aggiornato**
# JARs necessari per:
# - scrivere su MinIO tramite s3a:// (hadoop-aws + AWS SDK)
# - leggere da Kafka via Spark Structured Streaming

# spark.jars.dir /opt/config/spark/jars # Directory locale dove Spark cerca JAR aggiuntivi

# spark jar necessari per Kafka e S3A
# spark.jars=/opt/spark/jars/spark-sql-kafka-0-10_2.12-3.5.1.jar,\
# /opt/spark/jars/kafka-clients-3.5.1.jar,\
# /opt/spark/jars/spark-token-provider-kafka-0-10_2.12-3.5.1.jar,\
# /opt/spark/jars/hadoop-aws.jar,\
# /opt/spark/jars/aws-java-sdk-bundle.jar

spark.jars=/opt/spark/jars/spark-sql-kafka-0-10_2.12-4.0.0.jar,\
/opt/spark/jars/kafka-clients-3.6.0.jar,\
/opt/spark/jars/aws-java-sdk-bundle-1.11.1026.jar,\
/opt/spark/jars/spark-token-provider-kafka-0-10_2.12-4.0.0.jar,\
/opt/spark/jars/hadoop-aws.jar,\
/opt/spark/jars/jackson-core-2.14.2.jar,\
/opt/spark/jars/jackson-annotations-2.14.2.jar,\
/opt/spark/jars/jackson-databind-2.14.2.jar,\
/opt/spark/jars/parquet-hadoop-1.12.2.jar,\
/opt/spark/jars/parquet-common-1.12.2.jar,\
/opt/spark/jars/parquet-encoding-1.12.2.jar,\
/opt/spark/jars/parquet-column-1.12.2.jar


spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem


# Altre impostazioni Spark utili (facoltative ma consigliate)
# spark.sql.shuffle.partitions 4
# spark.executor.memory 1g
# spark.driver.memory 1g
